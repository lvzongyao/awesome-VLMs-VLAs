# Awesome Vision-Language-Models and Vision-Language-Action Models

> An awesome &amp; curated list of research papers and corresponding resources in vision-language-models and vision-language-action models
> 


## Taxonomy

- [Vision Foundation Models](#vfm)
- [Vision-Language-Models](#vlm)
- [Vision-Language-Action Models](#vla)
- [Failure Detection for VLAs](#fd_vla)
- [VLA + Reinforcement Learning](#vla_rl)
- [VLA + World Model](#vla_wm)
- [VLA for Autonomous Driving](#vla_ad)

***
<a name="vfm"></a>
## Vision Foundation Models
#### 2025
-
#### 2024
- Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (`CVPR 2024`). [[ğŸ“„paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf) [[ğŸ¤—model](https://huggingface.co/collections/microsoft/florence)]
#### 2021
- Florence: A New Foundation Model for Computer Vision. [[paper]](https://arxiv.org/pdf/2111.11432)
  
***
<a name="vlm"></a>
## Vision-Language-Models
#### 2025
-
#### 2024

***
<a name="vla"></a>
## Vision-Language-Action Models
#### 2025
- $\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization (`CoRL 2025`). [[paper]](https://openreview.net/pdf?id=vlhoswksBO) [[ğŸŒproject page]](https://www.pi.website/blog/pi05)
- FAST: Efficient Action Tokenization for Vision-Language-Action Models (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p012.pdf)
- $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p010.pdf) [[ğŸŒproject page]](https://www.physicalintelligence.company/blog/pi0)
- Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p017.pdf)
- RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation (`ICRA 2025`). [[paper]](https://arxiv.org/pdf/2411.02704) [[project page]](https://snasiriany.me/rt-affordance)
    
#### 2024
- Robotic Control via Embodied Chain-of-Thought Reasoning (`CoRL 2024`). [[ğŸ“„paper]](https://openreview.net/pdf?id=S70MgnIA0v) [[ğŸ’»code]](https://github.com/MichalZawalski/embodied-CoT) [[ğŸŒproject page]](https://embodied-cot.github.io) [[ğŸ¤—model]](https://huggingface.co/Embodied-CoT)
- OpenVLA: An Open-Source Vision-Language-Action Model (`CoRL 2024`). [[ğŸ“„paper]](https://openreview.net/pdf?id=ZMnD6QZAE6) [[ğŸ’»code]](https://github.com/openvla/openvla) [[ğŸŒproject page]](https://openvla.github.io) [[ğŸ¤—model]](https://huggingface.co/openvla)
- RoboPoint: A Vision-Language Model for Spatial Affordance Prediction in Robotics (`CoRL 2024`). [[ğŸ“„paper]](https://openreview.net/pdf?id=GVX6jpZOhU) [[ğŸ’»code]](https://github.com/wentaoyuan/RoboPoint) [[ğŸŒproject page]](https://robo-point.github.io/) [[ğŸ¤—model]](https://huggingface.co/wentao-yuan/robopoint-v1-vicuna-v1.5-13b)
- RT-H: Action Hierarchies Using Language (`RSS 2024`). [[paper]](https://www.roboticsproceedings.org/rss20/p049.pdf) [[project page]](https://rt-hierarchy.github.io)

#### 2023
- PaLM-E: An embodied multimodal language model (`ICML 2023`). [[ğŸ“„paper]](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf) [[ğŸŒproject page]](https://palm-e.github.io/)

#### 2022
- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (`CoRL 2022`). [[ğŸ“„paper]](https://openreview.net/pdf?id=bdHkMjBJG_w) [[ğŸŒproject page]](https://say-can.github.io/)

***
<a name="fd_vla"></a>
## Failure Detection for VLAs
#### 2025
- AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation (`ICLR 2025`). [[ğŸ“„paper]](https://openreview.net/pdf?id=JVkdSi7Ekg) [[ğŸ’»code]](https://github.com/NVlabs/AHA/tree/main) [[ğŸŒproject page]](https://aha-vlm.github.io/)
  
***
<a name="vla_rl"></a>
## VLA + Reinforcement Learning
#### 2025
- $\pi_{0.6}$: a VLA That Learns From Experience. [[paper]](https://www.pi.website/download/pistar06.pdf)
- $\pi_{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models. [[paper]](https://arxiv.org/pdf/2510.25889) [[ğŸ’»code]](https://github.com/RLinf/RLinf) [[ğŸ¤—model]](https://huggingface.co/RLinf)

***
<a name="vla_wm"></a>
## VLA + World Model
#### 2025
- RynnVLA-002: A Unified Vision-Language-Action and World Model. [[paper]](https://arxiv.org/pdf/2511.17502) [[ğŸ’»code]](https://github.com/alibaba-damo-academy/RynnVLA-002) [[ğŸ¤—model]](https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA)
- WorldVLA: Towards Autoregressive Action World Model. [[paper]](https://arxiv.org/pdf/2506.21539) [[ğŸ’»code]](https://github.com/alibaba-damo-academy/RynnVLA-002) [[ğŸ¤—Model]](https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002)

***
<a name="vla_ad"></a>
## VLA for Autonomous Driving
#### 2025


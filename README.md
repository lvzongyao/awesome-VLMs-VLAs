# Awesome Vision-Language-Models and Vision-Language-Action Models

> An awesome &amp; curated list of research papers and corresponding resources in vision-language-models and vision-language-action models
> 



## Taxonomy

- [Vision Foundation Models](#vfm)
- [Vision-Language-Models](#vlm)
- [Vision-Language-Action Models](#vla)
- [VLA + Reinforcement Learning](#vla_rl)
- [VLA + World Model](#vla_wm)
- [VLA for Autonomous Driving](#vla_ad)
- 

***
<a name="vfm"></a>
## Vision Foundation Models
#### 2025
-
#### 2024
- Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (`CVPR 2024`). [[üìÑpaper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf) [[ü§ómodel](https://huggingface.co/collections/microsoft/florence)]
#### 2021
- Florence: A New Foundation Model for Computer Vision. [[paper]](https://arxiv.org/pdf/2111.11432)
  
***
<a name="vlm"></a>
## Vision-Language-Models
#### 2025
-
#### 2024

***
<a name="vla"></a>
## Vision-Language-Action Models
#### 2025
- $\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization (`CoRL 2025`). [[paper]](https://openreview.net/pdf?id=vlhoswksBO) [[üåçproject page]](https://www.pi.website/blog/pi05)
- FAST: Efficient Action Tokenization for Vision-Language-Action Models (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p012.pdf)
- $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p010.pdf) [[üåçproject page]](https://www.physicalintelligence.company/blog/pi0)
- Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success (`RSS 2025`). [[paper]](https://www.roboticsproceedings.org/rss21/p017.pdf)
- RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation (`ICRA 2025`). [[paper]](https://arxiv.org/pdf/2411.02704) [[project page]](https://snasiriany.me/rt-affordance)
    
#### 2024
- RT-H: Action Hierarchies Using Language (`RSS 2024`). [[paper]](https://www.roboticsproceedings.org/rss20/p049.pdf) [[project page]](https://rt-hierarchy.github.io)

#### 2023
- PaLM-E: An embodied multimodal language model (`ICML 2023`). [[üìÑpaper]](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf) [[üåçproject page]](https://palm-e.github.io/)

#### 2022
- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (`CoRL 2022`). [[üìÑpaper]](https://openreview.net/pdf?id=bdHkMjBJG_w) [[üåçproject page]](https://say-can.github.io/)

***
<a name="vla_rl"></a>
## VLA + Reinforcement Learning
#### 2025
- $\pi_{0.6}$: a VLA That Learns From Experience. [[paper]](https://www.pi.website/download/pistar06.pdf)
- $\pi_{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models. [[paper]](https://arxiv.org/pdf/2510.25889) [[üíªcode]](https://github.com/RLinf/RLinf) [[ü§ómodel]](https://huggingface.co/RLinf)

***
<a name="vla_wm"></a>
## VLA + World Model
#### 2025
- RynnVLA-002: A Unified Vision-Language-Action and World Model. [[paper]](https://arxiv.org/pdf/2511.17502) [[üíªcode]](https://github.com/alibaba-damo-academy/RynnVLA-002) [[ü§ómodel]](https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA)
- WorldVLA: Towards Autoregressive Action World Model. [[paper]](https://arxiv.org/pdf/2506.21539) [[üíªcode]](https://github.com/alibaba-damo-academy/RynnVLA-002) [[ü§óModel]](https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002)

***
<a name="vla_ad"></a>
## VLA for Autonomous Driving
#### 2025

